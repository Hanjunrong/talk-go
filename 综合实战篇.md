# 综合实战篇

## 46 | 案例篇：为什么应用容器化后，启动慢了很多？

如果你在 Docker 容器中运行 Java 应用，一定要确保，在设置容器资源限制的同时，配置好 JVM 的资源选项（比如堆内存等）。当然，如果你可以升级 Java 版本，那么升级到 Java 10 ，就可以自动解决类似问题了。

当碰到容器化的应用程序性能时，你依然可以使用，我们前面讲过的各种方法来分析和定位。

* 只不过要记得，容器化后的性能分析，跟前面内容稍微有些区别，比如下面这几点。

* 容器本身通过 cgroups 进行资源隔离，所以，在分析时要考虑 cgroups 对应用程序的影响。

* 容器的文件系统、网络协议栈等跟主机隔离。虽然在容器外面，我们也可以分析容器的行为，不过有时候，进入容器的命名空间内部，可能更为方便。容器的运行可能还会依赖于其他组件，比如各种网络插件（比如 CNI）、存储插件（比如 CSI）、设备插件（比如 GPU）等，让容器的性能分析更加复杂。如果你需要分析容器性能，别忘了考虑它们对性能的影响。

## 47，48 | 案例篇：服务器总是时不时丢包，我该怎么办？

容器化后，应用程序会通过命名空间进行隔离。所以，你在分析时，不要忘了结合命名空间、cgroups、iptables 等来综合分析。比如：

* cgroups 会影响容器应用的运行；

* iptables 中的 NAT，会影响容器的网络性能；

* 叠加文件系统，会影响应用的 I/O 性能等。

![](https://static001.geekbang.org/resource/image/7d/1b/7d8cb9a2ce1c3bad4d74f46a632f671b.png)

![](https://static001.geekbang.org/resource/image/dd/fd/dd5b4050d555b1c23362456e357dfffd.png)

从图中你可以看出，可能发生丢包的位置，实际上贯穿了整个网络协议栈。换句话说，全程都有丢包的可能。比如我们从下往上看：

* 在两台 VM 连接之间，可能会发生传输失败的错误，比如网络拥塞、线路错误等；

* 在网卡收包后，环形缓冲区可能会因为溢出而丢包；

* 在链路层，可能会因为网络帧校验失败、QoS 等而丢包；

* 在 IP 层，可能会因为路由失败、组包大小超过 MTU 等而丢包；

* 在传输层，可能会因为端口未监听、资源占用超过内核限制等而丢包；

* 在套接字层，可能会因为套接字缓冲区溢出而丢包；

* 在应用层，可能会因为应用程序异常而丢包；

* 此外，如果配置了 iptables 规则，这些网络包也可能因为 iptables 过滤规则而丢包。

网络丢包，通常会带来严重的性能下降，特别是对 TCP 来说，丢包通常意味着网络拥塞和重传，进一步还会导致网络延迟增大、吞吐降低。

### iptables

首先我们要知道，除了网络层和传输层的各种协议，iptables 和内核的连接跟踪机制也可能会导致丢包。所以，这也是发生丢包问题时，我们必须要排查的一个因素。

回顾一下 iptables 的原理，它基于 Netfilter 框架，通过一系列的规则，对网络数据包进行过滤（如防火墙）和修改（如 NAT）。

这些 iptables 规则，统一管理在一系列的表中，包括 filter（用于过滤）、nat（用于 NAT）、mangle（用于修改分组数据） 和 raw（用于原始数据包）等。

而每张表又可以包括一系列的链，用于对 iptables 规则进行分组管理。

对于丢包问题来说，最大的可能就是被 filter 表中的规则给丢弃了。

要弄清楚这一点，就需要我们确认，那些目标为 DROP 和 REJECT 等会弃包的规则，有没有被执行到。

你可以把所有的 iptables 规则列出来，根据收发包的特点，跟 iptables 规则进行匹配。

不过显然，如果 iptables 规则比较多，这样做的效率就会很低。

当然，更简单的方法，就是直接查询 DROP 和 REJECT 等规则的统计信息，看看是否为 0。

如果统计值不是 0 ，再把相关的规则拎出来进行分析。

我们可以通过 iptables -nvL 命令，查看各条规则的统计信息。

比如，你可以执行下面的 docker exec 命令，进入容器终端；然后再执行下面的 iptables 命令，就可以看到 filter 表的统计数据了

`

# 在主机中执行
$ docker exec -it nginx bash

# 在容器中执行
root@nginx:/# iptables -t filter -nvL
Chain INPUT (policy ACCEPT 25 packets, 1000 bytes)
 pkts bytes target     prot opt in     out     source               destination
    6   240 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.29999999981

Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination

Chain OUTPUT (policy ACCEPT 15 packets, 660 bytes)
 pkts bytes target     prot opt in     out     source               destination
    6   264 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.29999999981
`

### tcpdump

从 tcpdump 的输出中，我们就可以看到：

前三个包是正常的 TCP 三次握手，这没问题；但第四个包却是在 3 秒以后了，并且还是客户端（VM2）发送过来的 FIN 包，也就说明，客户端的连接关闭了。

我想，根据 curl 设置的 3 秒超时选项，你应该能猜到，这是因为 curl 命令超时后退出了。

### 内核线程

Linux 在启动过程中，有三个特殊的进程，也就是 PID 号最小的三个进程。

* 0 号进程为 idle 进程，这也是系统创建的第一个进程，它在初始化 1 号和 2 号进程后，演变为空闲任务。当 CPU 上没有其他任务执行时，就会运行它。

* 1 号进程为 init 进程，通常是 systemd 进程，在用户态运行，用来管理其他用户态进程。

* 2 号进程为 kthreadd 进程，在内核态运行，用来管理内核线程。

kswapd0：用于内存回收。在  Swap 变高 案例中，我曾介绍过它的工作原理。

kworker：用于执行内核工作队列，分为绑定 CPU （名称格式为 kworker/CPU86330）和未绑定 CPU（名称格式为 kworker/uPOOL86330）两类。

migration：在负载均衡过程中，把进程迁移到 CPU 上。每个 CPU 都有一个 migration 内核线程。

jbd2/sda1-8：jbd 是 Journaling Block Device 的缩写，用来为文件系统提供日志功能，以保证数据的完整性；名称中的 sda1-8，表示磁盘分区名称和设备号。每个使用了 ext4 文件系统的磁盘分区，都会有一个 jbd2 内核线程。

pdflush：用于将内存中的脏页（被修改过，但还未写入磁盘的文件页）写入磁盘（已经在 3.10 中合并入了 kworker 中）。

## 49 | 案例篇：内核线程 CPU 利用率太高，我该怎么办？

### 火焰图

火焰图可以分为下面这几种。

on-CPU 火焰图：表示 CPU 的繁忙情况，用在 CPU 使用率比较高的场景中。

off-CPU 火焰图：表示 CPU 等待 I/O、锁等各种资源的阻塞情况。

内存火焰图：表示内存的分配和释放情况。

热 / 冷火焰图：表示将 on-CPU 和 off-CPU 结合在一起综合展示。

差分火焰图：表示两个火焰图的差分情况，红色表示增长，蓝色表示衰减。

差分火焰图常用来比较不同场景和不同时期的火焰图，以便分析系统变化前后对性能的影响情况。


当时，我们从软中断 CPU 使用率的角度入手，用网络抓包的方法找出了瓶颈来源，确认是测试机器发送的大量 SYN 包导致的。
而通过今天的 perf 和火焰图方法，我们进一步找出了软中断内核线程的热点函数，其实也就找出了潜在的瓶颈和优化方向。其实，如果遇到的是内核线程的资源使用异常，很多常用的进程级性能工具并不能帮上忙。这时，你就可以用内核自带的 perf 来观察它们的行为，找出热点函数，进一步定位性能瓶。当然，perf 产生的汇总报告并不够直观，所以我也推荐你用火焰图来协助排查。实际上，火焰图方法同样适用于普通进程。比如，在分析 Nginx、MySQL 等各种应用场景的性能问题时，火焰图也能帮你更快定位热点函数，找出潜在性能问题。

## 50，51 | 案例篇：动态追踪怎么用？

动态追踪技术，通过探针机制，来采集内核或者应用程序的运行信息，从而可以不用修改内核和应用程序的代码，就获得丰富的信息，帮你分析、定位想要排查的问题。

所谓动态追踪，就是在系统或应用程序正常运行时，通过内核中提供的探针来动态追踪它们的行为，从而辅助排查出性能瓶颈。而在 Linux 系统中，常见的动态追踪方法包括 ftrace、perf、eBPF 以及 SystemTap 等。当你已经定位了某个内核函数，但不清楚它的实现原理时，就可以用 ftrace 来跟踪它的执行过程

### perf

用到它，来查找应用程序或者内核中的热点函数，从而定位性能瓶颈。而在内核线程 CPU 高的案例中，我们还使用火焰图动态展示 perf 的事件记录，从而更直观地发现了问题。不过，我们前面使用 perf record/top 时，都是先对事件进行采样，然后再根据采样数，评估各个函数的调用频率。实际上，perf 的功能远不止于此。比如，

perf 可以用来分析 CPU cache、CPU 迁移、分支预测、指令周期等各种硬件事件；

perf 也可以只对感兴趣的事件进行动态追踪。
