# 33, 34 | 关于 Linux 网络，你必须知道这些

## 七层网络模型

* 应用层，负责为应用程序提供统一的接口。

* 表示层，负责把数据转换成兼容接收系统的格式。

* 会话层，负责维护计算机之间的通信连接。

* 传输层，负责为数据加上传输表头，形成数据包。

* 网络层，负责数据的路由和转发。

* 数据链路层，负责 MAC 寻址、错误侦测和改错。

* 物理层，负责在物理网络中传输数据帧。

在 Linux 中，我们实际上使用的是另一个更实用的四层模型，

## TCP/IP 网络模型

* 应用层，负责向用户提供一组应用程序，比如 HTTP、FTP、DNS 等。

* 传输层，负责端到端的通信，比如 TCP、UDP 等。

* 网络层，负责网络包的封装、寻址和路由，比如 IP、ICMP 等。

* 网络接口层，负责网络包在物理网络中的传输，比如 MAC 寻址、错误侦测以及通过网卡传输网络帧等。

![](https://static001.geekbang.org/resource/image/f2/bd/f2dbfb5500c2aa7c47de6216ee7098bd.png)

## Linux 网络栈

![](https://static001.geekbang.org/resource/image/c8/79/c8dfe80acc44ba1aa9df327c54349e79.png)

传输层在应用程序数据前面增加了 TCP 头；

网络层在 TCP 数据包前增加了 IP 头；

而网络接口层，又在 IP 数据包前后分别增加了帧头和帧尾。

## Linux 网络收发流程

### 网络包的接收流程

在链路层检查报文的合法性，找出上层协议的类型（比如 IPv4 还是 IPv6），再去掉帧头、帧尾，然后交给网络层。

网络层取出 IP 头，判断网络包下一步的走向，比如是交给上层处理还是转发。当网络层确认这个包是要发送到本机后，就会取出上层协议的类型（比如 TCP 还是 UDP），去掉 IP 头，再交给传输层处理。

传输层取出 TCP 头或者 UDP 头后，根据 < 源 IP、源端口、目的 IP、目的端口 > 四元组作为标识，找出对应的 Socket，并把数据拷贝到 Socket 的接收缓存中。

### 网络包的发送流程

与接收流程相反

## 性能指标

实际上，我们通常用带宽、吞吐量、延时、PPS（Packet Per Second）等指标衡量网络的性能。

* 带宽，表示链路的最大传输速率，单位通常为 b/s （比特 / 秒）。

* 吞吐量，表示单位时间内成功传输的数据量，单位通常为 b/s（比特 / 秒）或者 B/s（字节 / 秒）。吞吐量受带宽限制，而吞吐量 / 带宽，也就是该网络的使用率。

* 延时，表示从网络请求发出后，一直到收到远端响应，所需要的时间延迟。在不同场景中，这一指标可能会有不同含义。比如，它可以表示，建立连接需要的时间（比如 TCP 握手延时），或一个数据包往返所需的时间（比如 RTT）。PPS，是 Packet Per Second（包 / 秒）的缩写，表示以网络包为单位的传输速率。

* PPS 通常用来评估网络的转发能力，比如硬件交换机，通常可以达到线性转发（即 PPS 可以达到或者接近理论最大值）。而基于 Linux 服务器的转发，则容易受网络包大小的影响。

除了这些指标，网络的可用性（网络能否正常通信）、并发连接数（TCP 连接数量）、丢包率（丢包百分比）、重传率（重新传输的网络包比例）等也是常用的性能指标。

## 网络配置

分析网络问题的第一步，通常是查看网络接口的配置和状态。你可以使用 ifconfig 或者 ip 命令，来查看网络的配置

第一，网络接口的状态标志。ifconfig 输出中的 RUNNING ，或 ip 输出中的 LOWER_UP ，都表示物理网络是连通的，即网卡已经连接到了交换机或者路由器中。如果你看不到它们，通常表示网线被拔掉了。

第二，MTU 的大小。MTU 默认大小是 1500，根据网络架构的不同（比如是否使用了 VXLAN 等叠加网络），你可能需要调大或者调小 MTU 的数值。

第三，网络接口的 IP 地址、子网以及 MAC 地址。这些都是保障网络功能正常工作所必需的，你需要确保配置正确。

第四，网络收发的字节数、包数、错误数以及丢包情况，特别是 TX 和 RX 部分的 errors、dropped、overruns、carrier 以及 collisions 等指标不为 0 时，通常表示出现了网络 I/O 问题。其中：

* errors 表示发生错误的数据包数，比如校验错误、帧同步错误等；

* dropped 表示丢弃的数据包数，即数据包已经收到了 Ring Buffer，但因为内存不足等原因丢包；overruns 表示超限数据包数，即网络 I/O 速度过快，导致 Ring Buffer 中的数据包来不及处理（队列满）而导致的丢包；

* carrier 表示发生 carrirer 错误的数据包数，比如双工模式不匹配、物理电缆出现问题等；

* collisions 表示碰撞数据包数。

## 套接字信息

可以用 netstat 或者 ss ，来查看套接字、网络栈、网络接口以及路由表的信息。(推荐ss)

`$ ss -ltnp | head -n 3

State Recv-Q Send-Q Local Address:Port Peer Address:Port

LISTEN 0 128 127.0.0.53%lo:53 0.0.0.0:* users:(("systemd-resolve",pid=840,fd=13))

LISTEN 0 128 0.0.0.0:22 0.0.0.0:* users:(("sshd",pid=1459,fd=3))`

netstat 和 ss 的输出也是类似的，都展示了套接字的状态、接收队列、发送队列、本地地址、远端地址、进程 PID 和进程名称等。

## 协议栈统计信息

` $ netstat -s `

` $ ss -s `

## 网络吞吐和 PPS

给 sar 增加 -n 参数就可以查看网络的统计信息，比如网络接口（DEV）、网络接口错误（EDEV）、TCP、UDP、ICMP 等等。

` $ sar -n DEV 1 `

* rxpck/s 和 txpck/s 分别是接收和发送的 PPS，单位为包 / 秒。

* rxkB/s 和 txkB/s 分别是接收和发送的吞吐量，单位是 KB/ 秒。

* rxcmp/s 和 txcmp/s 分别是接收和发送的压缩数据包数，单位是包 / 秒。

* %ifutil 是网络接口的使用率，即半双工模式下为 (rxkB/s+txkB/s)/Bandwidth，而全双工模式下为 max(rxkB/s, txkB/s)/Bandwidth。

## 连通性和延时

通常使用 ping ，来测试远程主机的连通性和延时，而这基于 ICMP 协议

ping 的输出，可以分为两部分。

第一部分，是每个 ICMP 请求的信息，包括 ICMP 序列号（icmp_seq）、TTL（生存时间，或者跳数）以及往返延时。

第二部分，则是三次 ICMP 请求的汇总。

# 35 | 基础篇：C10K 和 C1000K 回顾

C10K 就是单机同时处理 1 万个请求（并发连接 1 万）的问题，而 C1000K 也就是单机支持处理 100 万个请求（并发连接 100 万）的问题。

## I/O模型优化

使用异步、非阻塞I/O的解决思路。即I/O多路复用

### 两种I/O事件通知的方式：

* 水平触发：只要文件描述符可以非阻塞的执行I/O，就会出发通知，也就是说，应用程序可以随时检查文件描述符的状态，然后再根据状态，进行操作

* 边缘触发：只有文件描述符的状态发生改变，才发送一次通知，这时程序需要尽可能多的执行I/O，直到无法继续读写，才可以停止，如果I/O没执行完，或者因为某种原因没来得及处理，那么这次通知也就丢失了

### 实现I/O多路复用的方法

使用非阻塞I/O和水平触发通知，使用select或者poll

由于I/O是非阻塞的，一个线程就可以同时监控一批套接字的文件描述符

优点：对应用程序比较友好，而且API非常简单。

缺点：当请求数量多的时候，轮训就会比较耗时。

select使用固定长度的位相量，表示文件描述符的集合，因此会有最大描述符数量的限制，在32位系统中，默认限制是1024，
并且select内部，检查套接字状态时轮训的方法，再加上应用软件使用时的轮训，就变成的O(n^2)的关系了，

poll改进了select的方法，换成了不固定长度的数组，没有了最大描述符的限制，但应用程序使用poll时候，同样需要对文件描述符列表进行轮训

应用程序每次调用select和poll时，还需要把文件描述符的集合，从用户空间传入内核空间，由内核修改后，再传出到用户空间，增加了处理成本

### 使用非阻塞I/O和边缘触发通知，epoll

特点：epoll使用红黑树，在内核管理文件描述符的集合，这样，就不需要应用程序在每次操作时传入、传出集合；使用事件驱动的机制，只关注有I/O事件发生的文件描述符，不需要轮训扫描整个集合

#### 使用异步 I/O（Asynchronous I/O，简称为 AIO）

使用难度比较高

## 工作模型优化

### 主进程+多个worker子进程

工作模式：

主进程执行bind（）+listen（）后，创建多个子进程

每个子进程中通过accept（）或epoll_wait()来处理相同的套接字

惊群问题：当accept（）或epoll_wait()调用时候，多个进程被同时唤醒，但实际只有一个进程来响应此次事件，其他的被唤醒的又会重新休眠

惊群问题 accept（）在linux2.6中解决，而epoll的问题，则子linux4.5才通过EPOLLEXCLUSIVE解决

nginx通过为每个worker进程中，增加一个全局锁（accept_mutex），这些worker进程需要首先竞争到锁，只有竞争到锁的进程，才会加入到epoll中，确保了只有一个子进程被唤醒

nginx性能好的原因：它的这些worker进程，并不会经常的创建和销毁，在没任务时候回休眠，有任务唤醒。只有某些异常退出时，才会重新创建新的worker子进程。可以使用线程替换进程的方式

### 监听到相同端口的多进程模型

所有进程监听相同的接口，并且开启了SO_REUSEPORT选项，由内核负责将请求负载均衡到这些监听进程中去。内核确保了只有一个进程被唤醒，不会出现惊群问题。这个需要在linux3.9以上版本才可行

## C1000K问题

本质上还是构建在epoll的非阻塞I/O模型之上，只不过除了I/O模型之外还要从应用程序到内核，再到cpu以及内存，网络等各个层次的深度优化。

### C10M问题

本质是：跳过内核协议栈的冗长路径，把网络包直接送到要处理的应用程序那里去，常见的机制有DPDK和XDP

DPDK：用户态网络的标准，它跳过内核协议栈，直接由用户态进程通过轮训的方式，来处理网络接收
在PPS非常高的场景中，查询时间比实际工作时间少了很多，绝大部分时间都在处理网络包
跳过内核协议栈后，省去了繁杂的硬中断，软中断，再到linux网络协议栈逐层处理的过程，应用程序可以针对应用的实际场景，有针对性的优化网络包的处理逻辑
通过大页，CPU绑定，内存对齐，流水线并发等多种机制，优化网络包的处理效率

XDP（eXpress Data Path）：由linux内核提供的高性能网络数据路径，它允许网络包，在进入内核协议栈之前，就进行处理，也可以带来更高的性能。对内核要求linux4.8以上版本。并且不提供缓存队列。基于XDP的应用程序通常是专用的网络应用，常见的又IDS，DDoS防御，cilium容器网络插件。

